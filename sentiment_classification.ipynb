{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP4/l7aqM9yJUMx60XmUJhn"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":39,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G3Z-9wkQd086","executionInfo":{"status":"ok","timestamp":1682703064264,"user_tz":240,"elapsed":4209,"user":{"displayName":"Yu An","userId":"02614991907063134700"}},"outputId":"df4d112c-ae57-466a-bd1e-1f71a8cab4f1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.28.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2023.4.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n"]}],"source":["pip install transformers"]},{"cell_type":"code","source":["import re\n","import os\n","\n","import numpy as np\n","import torch\n","from torch.nn import CrossEntropyLoss\n","from torch.utils.data import DataLoader\n","from torch.utils.data import RandomSampler\n","from tqdm import tqdm, trange\n","from transformers import AdamW, get_linear_schedule_with_warmup\n","from transformers import BertConfig, BertForSequenceClassification, BertTokenizer\n"],"metadata":{"id":"LWmxTuD2eSVS","executionInfo":{"status":"ok","timestamp":1682703064265,"user_tz":240,"elapsed":8,"user":{"displayName":"Yu An","userId":"02614991907063134700"}}},"execution_count":40,"outputs":[]},{"cell_type":"code","source":["PAD_TOKEN_LABEL_ID = CrossEntropyLoss().ignore_index\n","\n","BATCH_SIZE = 16\n","LEARNING_RATE_MODEL = 1e-5\n","LEARNING_RATE_CLASSIFIER = 1e-3\n","WARMUP_STEPS = 0\n","GRADIENT_ACCUMULATION_STEPS = 1\n","MAX_GRAD_NORM = 1.0\n","SEED = 42\n","NO_CUDA = False"],"metadata":{"id":"5GTNZK4eeXe-","executionInfo":{"status":"ok","timestamp":1682703064265,"user_tz":240,"elapsed":7,"user":{"displayName":"Yu An","userId":"02614991907063134700"}}},"execution_count":41,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","X_m = pd.read_csv(\"IMDB Dataset.csv\")\n","X = X_m.iloc[:25000]\n","for i in range (25000):\n","  if X['sentiment'].iloc[i] == 'positive':\n","    X['sentiment'].iloc[i] = 1\n","  else:\n","    X['sentiment'].iloc[i] = 0\n","data_train, X=data_test = train_test_split(X, test_size=0.3, random_state=42)\n","data_train.to_csv(\"imdb_train.csv\",index=False)\n","data_train.to_csv(\"imdb_test.csv\",index=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"URNrpGfPqERh","executionInfo":{"status":"ok","timestamp":1682703080538,"user_tz":240,"elapsed":16280,"user":{"displayName":"Yu An","userId":"02614991907063134700"}},"outputId":"33593bba-0743-4812-cade-aa745a3a420c"},"execution_count":42,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-42-c58f6b722a76>:8: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  X['sentiment'].iloc[i] = 1\n","<ipython-input-42-c58f6b722a76>:10: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  X['sentiment'].iloc[i] = 0\n"]}]},{"cell_type":"code","source":["print(X.columns)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KzM-jjpl8LS2","executionInfo":{"status":"ok","timestamp":1682703439111,"user_tz":240,"elapsed":188,"user":{"displayName":"Yu An","userId":"02614991907063134700"}},"outputId":"ce1d81d2-43be-44e0-8e75-e7f65f79238f"},"execution_count":46,"outputs":[{"output_type":"stream","name":"stdout","text":["Index(['review', 'sentiment'], dtype='object')\n"]}]},{"cell_type":"code","source":["def rpad(array, n):\n","    current_len = len(array)\n","    if current_len > n:\n","        return array[:n]\n","    extra = n - current_len\n","    return array + ([0] * extra)\n","\n","\n","def convert_to_embedding(tokenizer, sentences_with_labels):\n","    for i in range(len(sentences_with_labels.index)):\n","        sentence = sentences_with_labels['review'].iloc[i]\n","        label = sentences_with_labels['sentiment'].iloc[i]\n","        tokens = tokenizer.tokenize(sentence)\n","        tokens = tokens[:50]\n","        bert_sent = rpad(tokenizer.convert_tokens_to_ids([\"CLS\"] + tokens + [\"SEP\"]), n=256)\n","        yield torch.tensor(bert_sent), torch.tensor(label, dtype=torch.int64)\n","\n","def parse_line(line):\n","    line = line.strip().lower()\n","    line = line.replace(\"&nbsp;\", \" \")\n","    line = re.sub(r'<br(\\s\\/)?>', ' ', line)\n","    line = re.sub(r' +', ' ', line)  # merge multiple spaces into one\n","\n","    return line\n","\n","\n","def prepare_dataloader(tokenizer, sampler=RandomSampler, train=False):\n","    filename = \"imdb_train.csv\" if train else \"imdb_test.csv\"\n","    \n","    sentences_with_labels = pd.read_csv(filename)\n","\n","    dataset = list(convert_to_embedding(tokenizer, sentences_with_labels))\n","\n","    sampler_func = sampler(dataset) if sampler is not None else None\n","    dataloader = DataLoader(dataset, sampler=sampler_func, batch_size=BATCH_SIZE)\n","\n","    return dataloader"],"metadata":{"id":"DeqHIfckeb3b","executionInfo":{"status":"ok","timestamp":1682703481020,"user_tz":240,"elapsed":150,"user":{"displayName":"Yu An","userId":"02614991907063134700"}}},"execution_count":47,"outputs":[]},{"cell_type":"code","source":["class Transformers:\n","  model = None\n","\n","  def __init__(self, tokenizer):\n","      self.pad_token_label_id = PAD_TOKEN_LABEL_ID\n","      self.device = torch.device(\"cuda\" if torch.cuda.is_available() and not NO_CUDA else \"cpu\")\n","      self.tokenizer = tokenizer\n","\n","  def predict(self, sentence):\n","      if self.model is None or self.tokenizer is None:\n","          self.load()\n","\n","      embeddings = list(convert_to_embedding([(sentence, -1)]))\n","      preds = self._predict_tags_batched(embeddings)\n","      return preds\n","\n","  def evaluate(self, dataloader):\n","      from sklearn.metrics import classification_report\n","      y_pred = self._predict_tags_batched(dataloader)\n","      print(len(y_pred))\n","      dt = pd.read_csv(\"imdb_test.csv\")\n","      y_true = dt['sentiment']\n","      print(len(y_true))\n","\n","      score = classification_report(y_true, y_pred)\n","      print(score)\n","\n","  def _predict_tags_batched(self, dataloader):\n","        preds = []\n","        self.model.eval()\n","        for batch in tqdm(dataloader, desc=\"Computing NER tags\"):\n","            batch = tuple(t.to(self.device) for t in batch)\n","\n","            with torch.no_grad():\n","                outputs = self.model(batch[0])\n","                _, is_neg = torch.max(outputs[0], 1)\n","                preds.extend(is_neg.cpu().detach().numpy())\n","\n","        return preds\n","  \n","  def train(self, dataloader, model, epochs):\n","      assert self.model is None  # make sure we are not training after load() command\n","      model.to(self.device)\n","      self.model = model\n","\n","      t_total = len(dataloader) // GRADIENT_ACCUMULATION_STEPS * epochs\n","\n","      # Prepare optimizer and schedule (linear warmup and decay)\n","      optimizer_grouped_parameters = [\n","          {\"params\": model.bert.parameters(), \"lr\": LEARNING_RATE_MODEL},\n","          {\"params\": model.classifier.parameters(), \"lr\": LEARNING_RATE_CLASSIFIER}\n","      ]\n","      optimizer = AdamW(optimizer_grouped_parameters)\n","      scheduler = get_linear_schedule_with_warmup(\n","          optimizer, num_warmup_steps=WARMUP_STEPS, num_training_steps=t_total)\n","      \n","      # Train!\n","      print(\"***** Running training *****\")\n","      print(\"Training on %d examples\", len(dataloader))\n","      print(\"Num Epochs = %d\", epochs)\n","      print(\"Total optimization steps = %d\", t_total)\n","      \n","      global_step = 0\n","      tr_loss, logging_loss = 0.0, 0.0\n","      model.zero_grad()\n","      train_iterator = trange(epochs, desc=\"Epoch\")\n","      self._set_seed()\n","      for _ in train_iterator:\n","          epoch_iterator = tqdm(dataloader, desc=\"Iteration\")\n","          for step, batch in enumerate(epoch_iterator):\n","              model.train()\n","              batch = tuple(t.to(self.device) for t in batch)\n","              outputs = model(batch[0], labels=batch[1])\n","              loss = outputs[0]  # model outputs are always tuple in pytorch-transformers (see doc)\n","\n","              if GRADIENT_ACCUMULATION_STEPS > 1:\n","                  loss = loss / GRADIENT_ACCUMULATION_STEPS\n","\n","              loss.backward()\n","              tr_loss += loss.item()\n","              if (step + 1) % GRADIENT_ACCUMULATION_STEPS == 0:\n","                torch.nn.utils.clip_grad_norm_(model.parameters(), MAX_GRAD_NORM)\n","\n","                scheduler.step()  # Update learning rate schedule\n","                optimizer.step()\n","                model.zero_grad()\n","                global_step += 1\n","      self.model = model\n","\n","      return global_step, tr_loss / global_step\n","\n","\n","  def _set_seed(self):\n","      torch.manual_seed(SEED)\n","      if self.device == 'gpu':\n","          torch.cuda.manual_seed_all(SEED)\n","\n","  def load(self, model_dir='weights/'):\n","      self.tokenizer = BertTokenizer.from_pretrained(model_dir)\n","      self.model = BertForSequenceClassification.from_pretrained(model_dir)\n","      self.model.to(self.device)\n","  "],"metadata":{"id":"zzTO4YQyezRV","executionInfo":{"status":"ok","timestamp":1682705565739,"user_tz":240,"elapsed":588,"user":{"displayName":"Yu An","userId":"02614991907063134700"}}},"execution_count":51,"outputs":[]},{"cell_type":"code","source":["def train(epochs=20, output_dir=\"weights/\"):\n","    num_labels = 2  # negative and positive reviews\n","    config = BertConfig.from_pretrained('bert-base-uncased', num_labels=num_labels)\n","    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n","    model = BertForSequenceClassification.from_pretrained('bert-base-uncased', config=config)\n","\n","    dataloader = prepare_dataloader(tokenizer, train=True)\n","    predictor = Transformers(tokenizer)\n","    predictor.train(dataloader, model, epochs)\n","\n","    model.save_pretrained(output_dir)\n","    tokenizer.save_pretrained(output_dir)\n","\n","def evaluate(model_dir=\"weights/\"):\n","    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n","\n","    dataloader = prepare_dataloader(tokenizer, train=False, sampler=None)\n","    predictor = Transformers(tokenizer)\n","    predictor.load(model_dir=model_dir)\n","    predictor.evaluate(dataloader)\n","\n","path = 'weights/'\n","#os.makedirs(path, exist_ok=True)\n","#train(epochs=1, output_dir=path)\n","evaluate(model_dir=path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Bsey2JCHf-gH","executionInfo":{"status":"ok","timestamp":1682705931666,"user_tz":240,"elapsed":361611,"user":{"displayName":"Yu An","userId":"02614991907063134700"}},"outputId":"553827ef-a569-424e-fbe4-a417a1ba6029"},"execution_count":52,"outputs":[{"output_type":"stream","name":"stderr","text":["Computing NER tags: 100%|██████████| 1094/1094 [04:24<00:00,  4.14it/s]\n"]},{"output_type":"stream","name":"stdout","text":["17500\n","17500\n","              precision    recall  f1-score   support\n","\n","           0       0.67      0.61      0.64      8760\n","           1       0.64      0.70      0.67      8740\n","\n","    accuracy                           0.66     17500\n","   macro avg       0.66      0.66      0.66     17500\n","weighted avg       0.66      0.66      0.66     17500\n","\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"WBTgL-2kD8Pw"},"execution_count":null,"outputs":[]}]}